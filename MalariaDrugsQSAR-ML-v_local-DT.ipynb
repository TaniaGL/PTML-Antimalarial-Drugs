{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_8Th9MmQg2i"
   },
   "source": [
    "# QSAR for anti-malaria drugs\n",
    "\n",
    "Dataset was obtained from Chembl database. The project will search for the best QSAR models as a relationship between the input features and the output class (anti-malaria or not). The model input features are obtained as moving averages (MAs) of the original Chembl drug features in specific experimental conditions. The current dataset has already the final MA descriptors. The dataset was cleaned for duplicate rows and the rows where shuffled.\n",
    "\n",
    "Input specific libraries for the calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "llScbmi_Qg2l"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# remove warnings\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pt13FgXkQg2p"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score,f1_score, recall_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFECV, VarianceThreshold, SelectKBest, chi2\n",
    "from sklearn.feature_selection import SelectFromModel, SelectPercentile, f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQBhNszeQg2s"
   },
   "source": [
    "Define few global variables (name of class column, the seed for random generator):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oNUu4fTRQg2t"
   },
   "outputs": [],
   "source": [
    "outVar = 'Class'\n",
    "seed = 42  # To ensure that it is always the same split\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "An0hgXZGQg2x"
   },
   "source": [
    "### Read dataset\n",
    "\n",
    "Read dataset from **datasets** folder as CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hp4DE3kUQg2y",
    "outputId": "814ef71b-fe57-42a9-bfbf-be01fb212b81"
   },
   "outputs": [],
   "source": [
    "sFile = './datasets/ds.'+str(outVar)+'.csv'\n",
    "print('\\n-> Read dataset', sFile)\n",
    "df = pd.read_csv(sFile)  # To read as DataFrame (data and header)\n",
    "print('* Columns:',list(df.columns))  # Print the names of the columns to verify\n",
    "print('* Dimension:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEiH0rvlQg21"
   },
   "source": [
    "### Dataset split in train and test subsets\n",
    "\n",
    "Split dataset into 80% train and 20% test subsets using class stratification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4mBWnIWBQg22",
    "outputId": "d84239c9-68dc-4a22-b35f-148f6492aa60"
   },
   "outputs": [],
   "source": [
    "X_data = df.drop(outVar, axis=1).values  # Remove the output and we are left with the columns of features (array)\n",
    "Y_data = df[outVar].values \n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X_data, Y_data,\n",
    "                                          random_state=seed, test_size=0.20,\n",
    "                                          stratify=Y_data)  # To take into account the proportion of classes in each split\n",
    "print(\"Dimensions for splits:\\n\", 'X_tr.shape =', X_tr.shape, 'X_ts.shape =', X_ts.shape,\n",
    "      'y_tr.shape =', y_tr.shape, 'y_ts.shape =', y_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOv1ZhvlQg25",
    "outputId": "1b0834bf-95a2-4bef-b947-72020e72beb0"
   },
   "outputs": [],
   "source": [
    "print(\"Splits:\", X_tr.shape, X_ts.shape, y_tr.shape, y_ts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ly4CpGd2Qg28"
   },
   "source": [
    "### Normalize dataset with values between 0 and 1\n",
    "\n",
    "Because the probability feature is already between 0 and 1, we used the same normalization range for all the features. The normalization used only the training values and it was applied to the test subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60gvHdeZQg28"
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_tr_norm = scaler.fit_transform(X_tr)\n",
    "X_ts_norm = scaler.transform(X_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lqN1dN19Qg3A"
   },
   "source": [
    "Save train subset as file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RiwebWu2Qg3B"
   },
   "outputs": [],
   "source": [
    "df_tr_norm = pd.DataFrame(X_tr_norm, columns = list(df.columns)[:-1])\n",
    "df_tr_norm['Class'] = y_tr\n",
    "df_tr_norm.shape\n",
    "df_tr_norm.to_csv(r'datasets\\ds.Class.tr.norm.csv',index=False)  # Maybe have to change the route. Index false to doesn't put an index column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cj5qffODQg3D"
   },
   "source": [
    "Save test subset as file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OivYRtp-Qg3E"
   },
   "outputs": [],
   "source": [
    "df_ts_norm = pd.DataFrame(X_ts_norm, columns = list(df.columns)[:-1])\n",
    "df_ts_norm['Class'] = y_ts\n",
    "df_ts_norm.shape\n",
    "df_ts_norm.to_csv(r'datasets\\ds.Class.ts.norm.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lRlpVvFuQg3G"
   },
   "source": [
    "### ML with training and test subsets\n",
    "\n",
    "Read train and test subsets as dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xdaxBxcrQg3H",
    "outputId": "cf77b600-6ea3-4609-d54d-6c1eea34aa31"
   },
   "outputs": [],
   "source": [
    "# Read tr and ts datasets\n",
    "df_tr_norm = pd.read_csv(r'datasets\\ds.Class.tr.norm.csv')\n",
    "df_ts_norm = pd.read_csv(r'datasets\\ds.Class.ts.norm.csv')\n",
    "print('Training shape:',df_tr_norm.shape)\n",
    "print('Test shape:',df_ts_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "niS5QU3UQg3K"
   },
   "source": [
    "Get data only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZCXEFbyvQg3K"
   },
   "outputs": [],
   "source": [
    "X_tr_norm = df_tr_norm.drop(outVar, axis=1).values\n",
    "y_tr_norm = df_tr_norm[outVar].values\n",
    "X_ts_norm = df_ts_norm.drop(outVar, axis=1).values\n",
    "y_ts_norm = df_ts_norm[outVar].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apHTTGRNQg3N"
   },
   "source": [
    "Define a function for ML with a single method and statistics such as ACC, AUROC, precision, recall, f1score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OdNxCGL7Qg3N"
   },
   "outputs": [],
   "source": [
    "def ML_baseline(cls, X_tr, y_tr, X_ts, y_ts, seed=42, classes=['0','1']):\n",
    "    ACC = 0\n",
    "    AUROC = 0\n",
    "    precision = 0 \n",
    "    recall = 0\n",
    "    f1score = 0\n",
    "    \n",
    "    cls_name = type(cls).__name__\n",
    "    \n",
    "    start_time = time.time()\n",
    "    cls.fit(X_tr, y_tr)\n",
    "    print('>', cls_name, \"training: %0.2f mins \" % ((time.time() - start_time)/60))\n",
    "    \n",
    "    # predictions\n",
    "    y_pred  = cls.predict(X_ts)\n",
    "    y_probs = cls.predict_proba(X_ts)[:, 1]\n",
    "    cls_rep = classification_report(y_ts, y_pred, target_names=classes,\n",
    "                                    output_dict=True, digits=3)\n",
    "    print(cls_rep)\n",
    "    \n",
    "    ACC       = accuracy_score(y_ts, y_pred)\n",
    "    AUROC     = roc_auc_score(y_ts, y_probs)\n",
    "    precision = cls_rep['weighted avg']['precision']\n",
    "    recall    = cls_rep['weighted avg']['recall']\n",
    "    f1score   = cls_rep['weighted avg']['f1-score']  \n",
    "    \n",
    "    \n",
    "    return ACC, AUROC, precision, recall, f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m5C41o1nQg3T"
   },
   "source": [
    "Define a function to return a dictionary with the class ballance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oA9YUYLmQg3U"
   },
   "outputs": [],
   "source": [
    "def  set_weights(y_data, option='balanced'):\n",
    "    \"\"\"Estimate class weights for umbalanced dataset\n",
    "       If ‘balanced’, class weights will be given by n_samples / (n_classes * np.bincount(y)). \n",
    "       If a dictionary is given, keys are classes and values are corresponding class weights. \n",
    "       If None is given, the class weights will be uniform \"\"\"\n",
    "    cw = class_weight.compute_class_weight(option, np.unique(y_data), y_data)\n",
    "    w = {i:j for i,j in zip(np.unique(y_data), cw)}\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ffPUltdkQg3X",
    "outputId": "08224ccb-591e-47f6-f260-ece7f67ce1af"
   },
   "outputs": [],
   "source": [
    "class_weights = set_weights(list(y_tr_norm), option='balanced')\n",
    "print('* Class ballance in training set:', class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3R-G5PEOQg3a"
   },
   "source": [
    "Define the list of classifiers for our ML baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-K5g6hELQg3b"
   },
   "outputs": [],
   "source": [
    "# Priors for LDA\n",
    "priors = [(class_weights[0]/(class_weights[0]+class_weights[1])), \n",
    "          (class_weights[1]/(class_weights[0]+class_weights[1]))]\n",
    "    \n",
    "classifiers = [#GaussianNB(),\n",
    "               #KNeighborsClassifier(n_jobs=-1, n_neighbors=3),  # n_jobs = -1 uses all cores, for n core(s) = n\n",
    "               #LinearDiscriminantAnalysis(solver='svd',priors=priors),  # Not have random_state\n",
    "               #SVC(kernel=\"linear\",random_state=seed,gamma='scale',class_weight=class_weights),\n",
    "               #SVC(kernel = 'rbf', random_state=seed,gamma='scale',class_weight=class_weights),\n",
    "               #LogisticRegression(solver='lbfgs',random_state=seed,class_weight=class_weights), \n",
    "               #MLPClassifier(hidden_layer_sizes= (20), random_state = seed, max_iter=50000, shuffle=False),  # Neurons should be at least 25 (number of items we work with)\n",
    "               DecisionTreeClassifier(random_state=seed,class_weight=class_weights),  # Only 1 tree, default: criterion = 'gini', splitter = 'best'\n",
    "               DecisionTreeClassifier(random_state=seed,class_weight=class_weights, max_depth=2),\n",
    "               DecisionTreeClassifier(random_state=seed,class_weight=class_weights, max_depth=6),\n",
    "               DecisionTreeClassifier(random_state=seed,class_weight=class_weights, max_depth=8),\n",
    "               DecisionTreeClassifier(random_state=seed,class_weight=class_weights, max_depth=10),\n",
    "               DecisionTreeClassifier(random_state=seed,class_weight=class_weights, max_depth=12),\n",
    "               DecisionTreeClassifier(criterion='gini', splitter='random', random_state=seed,class_weight=class_weights),\n",
    "               DecisionTreeClassifier(criterion='entropy', splitter='random', random_state=seed,class_weight=class_weights),\n",
    "               DecisionTreeClassifier(criterion='entropy', splitter='best', random_state=seed,class_weight=class_weights),\n",
    "               #RandomForestClassifier(n_estimators=100,n_jobs=-1,random_state=seed,class_weight=class_weights), \n",
    "               #GradientBoostingClassifier(random_state=seed), \n",
    "               #AdaBoostClassifier(random_state = seed), \n",
    "               #BaggingClassifier(random_state=seed)\n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3XGnXEEQg3k"
   },
   "source": [
    "Create a dataframe for the results with all ML statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ngLhOiqHQg3k"
   },
   "outputs": [],
   "source": [
    "# Create a dataframe for ML baseline\n",
    "df_ML = pd.DataFrame(columns=['Method', 'ACC','AUROC' ,'precision' ,'recall' ,'f1-score' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6dUVDkO-Qg3o"
   },
   "source": [
    "Fit each classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mBBcnUpSQg3p",
    "outputId": "43e2b84a-ecde-4960-c7ac-a0693a6d3871"
   },
   "outputs": [],
   "source": [
    "start_time_2 = time.time()\n",
    "# Fit each classifier\n",
    "for cls in classifiers:\n",
    "    print(\"\\n***\", cls)\n",
    "    ACC,AUROC,precision,recall,f1score=ML_baseline(cls, X_tr_norm, y_tr_norm, X_ts_norm, y_ts_norm, seed=seed)\n",
    "    df_ML = df_ML.append({'Method': str(type(cls).__name__),\n",
    "                          'ACC': float(ACC),\n",
    "                          'AUROC': float(AUROC),\n",
    "                          'precision': float(precision),\n",
    "                          'recall': float(recall),\n",
    "                          'f1-score': float(f1score)}, ignore_index=True)\n",
    "\n",
    "print(\"\\n >>> Total time: %0.2f mins \" % ((time.time() - start_time_2)/60))  # Running time for all selected methods\n",
    "df_ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4MriA-LQg3s"
   },
   "source": [
    "Save the results as CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QdFo3euWQg3t"
   },
   "outputs": [],
   "source": [
    "df_ML.to_csv(r'results\\ML_statistics.local.DT.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UIbY3JKbQg3w"
   },
   "source": [
    "..............."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of MalariaDrugsQSAR-ML-con_anotaciones-vlocal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}